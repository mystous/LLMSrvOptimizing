INFO 06-29 14:18:56 [__init__.py:243] Automatically detected platform cuda.
Namespace(backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=130, model='deepseek-ai/DeepSeek-R1-Distill-Llama-70B', tokenizer=None, use_beam_search=False, num_prompts=200, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, save_detailed=False, append_result=False, metadata=['input_len=512', 'output_len=512', 'serving=vllm'], result_dir='gpu_log', result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 130
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<02:25,  1.37it/s]  1%|          | 2/200 [00:01<02:23,  1.38it/s]  2%|▏         | 3/200 [00:01<01:32,  2.13it/s]  2%|▎         | 5/200 [00:01<00:53,  3.63it/s]  4%|▎         | 7/200 [00:02<00:35,  5.44it/s]  4%|▍         | 9/200 [00:02<00:42,  4.48it/s]  5%|▌         | 10/200 [00:03<00:55,  3.40it/s]  6%|▌         | 11/200 [00:03<00:56,  3.33it/s]  6%|▌         | 12/200 [00:03<00:55,  3.39it/s]  6%|▋         | 13/200 [00:04<01:08,  2.71it/s]  7%|▋         | 14/200 [00:04<00:58,  3.16it/s]  8%|▊         | 15/200 [00:04<00:57,  3.21it/s]  8%|▊         | 16/200 [00:04<00:50,  3.66it/s]  8%|▊         | 17/200 [00:07<02:33,  1.19it/s]  9%|▉         | 18/200 [00:07<02:12,  1.37it/s] 10%|▉         | 19/200 [00:08<01:52,  1.61it/s] 10%|█         | 20/200 [00:08<01:34,  1.90it/s] 10%|█         | 21/200 [00:08<01:39,  1.80it/s] 11%|█         | 22/200 [00:09<01:19,  2.24it/s] 12%|█▏        | 23/200 [00:09<01:08,  2.59it/s] 12%|█▏        | 24/200 [00:10<01:42,  1.71it/s] 12%|█▎        | 25/200 [00:10<01:18,  2.22it/s] 25%|██▌       | 50/200 [00:10<00:06, 23.70it/s] 54%|█████▎    | 107/200 [00:10<00:01, 80.60it/s] 66%|██████▋   | 133/200 [00:10<00:00, 93.43it/s] 75%|███████▌  | 150/200 [00:14<00:02, 18.46it/s] 81%|████████  | 162/200 [00:18<00:04,  8.68it/s] 86%|████████▋ | 173/200 [00:18<00:02, 10.62it/s]100%|██████████| 200/200 [00:18<00:00, 10.53it/s]
============ Serving Benchmark Result ============
Successful requests:                     200       
Benchmark duration (s):                  18.99     
Total input tokens:                      102200    
Total generated tokens:                  92784     
Request throughput (req/s):              10.53     
Output token throughput (tok/s):         4885.73   
Total Token throughput (tok/s):          10267.28  
---------------Time to First Token----------------
Mean TTFT (ms):                          270.82    
Median TTFT (ms):                        290.13    
P99 TTFT (ms):                           454.79    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          19.03     
Median TPOT (ms):                        20.11     
P99 TPOT (ms):                           20.59     
---------------Inter-token Latency----------------
Mean ITL (ms):                           19.02     
Median ITL (ms):                         19.44     
P99 ITL (ms):                            30.19     
==================================================
