INFO 06-29 20:42:40 [__init__.py:243] Automatically detected platform cuda.
Namespace(backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=90, model='deepseek-ai/DeepSeek-R1-Distill-Llama-70B', tokenizer=None, use_beam_search=False, num_prompts=300, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, save_detailed=False, append_result=False, metadata=['input_len=512', 'output_len=512', 'serving=vllm'], result_dir='gpu_log', result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 90
  0%|          | 0/300 [00:00<?, ?it/s]  0%|          | 1/300 [00:00<02:49,  1.76it/s]  1%|          | 2/300 [00:01<03:10,  1.56it/s]  1%|          | 3/300 [00:01<02:17,  2.16it/s]  1%|▏         | 4/300 [00:01<01:43,  2.86it/s]  2%|▏         | 5/300 [00:01<01:21,  3.64it/s]  2%|▏         | 6/300 [00:03<02:55,  1.68it/s]  2%|▏         | 7/300 [00:03<02:18,  2.12it/s]  3%|▎         | 8/300 [00:05<05:14,  1.08s/it]  3%|▎         | 9/300 [00:05<03:49,  1.27it/s]  3%|▎         | 10/300 [00:06<03:52,  1.25it/s]  4%|▎         | 11/300 [00:06<03:10,  1.52it/s]  4%|▍         | 12/300 [00:07<02:40,  1.79it/s]  4%|▍         | 13/300 [00:07<02:25,  1.98it/s]  5%|▍         | 14/300 [00:08<02:20,  2.03it/s]  5%|▌         | 15/300 [00:09<03:41,  1.29it/s]  5%|▌         | 16/300 [00:09<02:43,  1.74it/s] 27%|██▋       | 80/300 [00:09<00:04, 45.68it/s] 33%|███▎      | 98/300 [00:11<00:07, 28.62it/s] 37%|███▋      | 111/300 [00:15<00:18, 10.17it/s] 40%|████      | 120/300 [00:17<00:24,  7.29it/s] 42%|████▏     | 127/300 [00:19<00:24,  6.93it/s] 62%|██████▏   | 185/300 [00:19<00:05, 20.16it/s] 68%|██████▊   | 205/300 [00:20<00:05, 18.09it/s] 73%|███████▎  | 220/300 [00:25<00:08,  8.97it/s] 77%|███████▋  | 230/300 [00:28<00:09,  7.21it/s] 81%|████████  | 243/300 [00:28<00:06,  9.26it/s] 93%|█████████▎| 279/300 [00:28<00:01, 17.45it/s] 99%|█████████▊| 296/300 [00:29<00:00, 16.45it/s]100%|██████████| 300/300 [00:30<00:00,  9.87it/s]
============ Serving Benchmark Result ============
Successful requests:                     300       
Benchmark duration (s):                  30.39     
Total input tokens:                      153300    
Total generated tokens:                  136776    
Request throughput (req/s):              9.87      
Output token throughput (tok/s):         4501.38   
Total Token throughput (tok/s):          9546.58   
---------------Time to First Token----------------
Mean TTFT (ms):                          142.36    
Median TTFT (ms):                        148.30    
P99 TTFT (ms):                           292.45    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          18.23     
Median TPOT (ms):                        18.31     
P99 TPOT (ms):                           18.77     
---------------Inter-token Latency----------------
Mean ITL (ms):                           18.22     
Median ITL (ms):                         18.07     
P99 ITL (ms):                            27.96     
==================================================
