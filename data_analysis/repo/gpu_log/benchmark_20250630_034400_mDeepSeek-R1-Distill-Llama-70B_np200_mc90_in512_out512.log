INFO 06-30 03:44:05 [__init__.py:243] Automatically detected platform cuda.
Namespace(backend='vllm', base_url='http://127.0.0.1:8000', host='127.0.0.1', port=8000, endpoint='/v1/completions', dataset_name='random', dataset_path=None, max_concurrency=90, model='deepseek-ai/DeepSeek-R1-Distill-Llama-70B', tokenizer=None, use_beam_search=False, num_prompts=200, logprobs=None, request_rate=inf, burstiness=1.0, seed=0, trust_remote_code=False, disable_tqdm=False, profile=False, save_result=True, save_detailed=False, append_result=False, metadata=['input_len=512', 'output_len=512', 'serving=sglang'], result_dir='gpu_log', result_filename=None, ignore_eos=False, percentile_metrics='ttft,tpot,itl', metric_percentiles='99', goodput=None, custom_output_len=256, custom_skip_chat_template=False, sonnet_input_len=550, sonnet_output_len=150, sonnet_prefix_len=200, sharegpt_output_len=None, random_input_len=512, random_output_len=512, random_range_ratio=0.0, random_prefix_len=0, hf_subset=None, hf_split=None, hf_output_len=None, top_p=None, top_k=None, min_p=None, temperature=None, tokenizer_mode='auto', served_model_name=None, lora_modules=None)
Starting initial single prompt test run...
Initial test run completed. Starting main benchmark run...
Traffic request rate: inf
Burstiness factor: 1.0 (Poisson process)
Maximum request concurrency: 90
  0%|          | 0/200 [00:00<?, ?it/s]  0%|          | 1/200 [00:00<02:45,  1.20it/s]  1%|          | 2/200 [00:01<03:02,  1.09it/s]  2%|▏         | 3/200 [00:01<01:53,  1.73it/s]  2%|▏         | 4/200 [00:02<01:55,  1.69it/s]  2%|▎         | 5/200 [00:02<01:38,  1.99it/s]  4%|▎         | 7/200 [00:03<00:55,  3.47it/s]  4%|▍         | 8/200 [00:03<01:04,  2.97it/s]  5%|▌         | 10/200 [00:03<00:48,  3.94it/s]  6%|▌         | 11/200 [00:05<02:02,  1.55it/s]  6%|▌         | 12/200 [00:06<02:20,  1.34it/s]  6%|▋         | 13/200 [00:07<01:53,  1.65it/s]  7%|▋         | 14/200 [00:07<01:30,  2.05it/s]  8%|▊         | 15/200 [00:08<02:03,  1.50it/s]  8%|▊         | 16/200 [00:09<02:04,  1.48it/s] 48%|████▊     | 95/200 [00:09<00:02, 37.02it/s] 50%|█████     | 100/200 [00:10<00:03, 25.86it/s] 52%|█████▏    | 104/200 [00:11<00:05, 18.83it/s] 54%|█████▎    | 107/200 [00:11<00:06, 14.79it/s] 55%|█████▍    | 109/200 [00:12<00:06, 14.52it/s] 56%|█████▌    | 111/200 [00:12<00:08, 10.35it/s] 56%|█████▋    | 113/200 [00:14<00:17,  5.08it/s] 57%|█████▋    | 114/200 [00:15<00:22,  3.82it/s] 57%|█████▊    | 115/200 [00:15<00:21,  3.95it/s] 58%|█████▊    | 117/200 [00:16<00:17,  4.71it/s] 59%|█████▉    | 118/200 [00:16<00:18,  4.50it/s] 60%|█████▉    | 119/200 [00:16<00:19,  4.09it/s] 60%|██████    | 120/200 [00:17<00:22,  3.60it/s] 62%|██████▏   | 123/200 [00:17<00:18,  4.17it/s] 62%|██████▏   | 124/200 [00:17<00:16,  4.57it/s] 94%|█████████▍| 189/200 [00:18<00:00, 65.41it/s]100%|██████████| 200/200 [00:20<00:00, 21.84it/s]100%|██████████| 200/200 [00:20<00:00, 10.00it/s]
============ Serving Benchmark Result ============
Successful requests:                     200       
Benchmark duration (s):                  20.00     
Total input tokens:                      102200    
Total generated tokens:                  91938     
Request throughput (req/s):              10.00     
Output token throughput (tok/s):         4595.80   
Total Token throughput (tok/s):          9704.58   
---------------Time to First Token----------------
Mean TTFT (ms):                          339.65    
Median TTFT (ms):                        277.19    
P99 TTFT (ms):                           570.12    
-----Time per Output Token (excl. 1st token)------
Mean TPOT (ms):                          16.69     
Median TPOT (ms):                        16.64     
P99 TPOT (ms):                           18.61     
---------------Inter-token Latency----------------
Mean ITL (ms):                           16.64     
Median ITL (ms):                         15.63     
P99 ITL (ms):                            43.19     
==================================================
